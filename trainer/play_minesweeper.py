# play_minesweeper.py
import random
import numpy as np
import torch
import torch.nn as nn
from env_DQN import MineSweeperEnv

# åŠ è½½DQNæ¨¡å‹å’Œä»£ç†
class DQN(nn.Module):
    def __init__(self, input_size, output_size):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(input_size, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, output_size)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        return x

class DQNAgent:
    def __init__(self, input_size, output_size):
        self.input_size = input_size
        self.output_size = output_size
        self.policy_net = DQN(input_size, output_size)
    
    def load_model(self, path):
        self.policy_net.load_state_dict(torch.load(path))
        self.policy_net.eval()
        print(f"Model loaded from {path}")
    
    def select_action(self, state):
        # åˆ›å»ºæœ‰æ•ˆåŠ¨ä½œæ©ç ï¼ˆ-2 è¡¨ç¤ºæœªæ­å¼€çš„æ ¼å­ï¼‰
        valid_mask = (state == -2)
        
        # å¦‚æœæ²¡æœ‰æœ‰æ•ˆåŠ¨ä½œï¼Œéšæœºé€‰æ‹©
        if not np.any(valid_mask):
            return random.randint(0, self.output_size - 1)
        
        with torch.no_grad():
            state_tensor = torch.tensor(state, dtype=torch.float32)
            q_values = self.policy_net(state_tensor)
            
            # å°†æ— æ•ˆåŠ¨ä½œçš„Qå€¼è®¾ä¸ºè´Ÿæ— ç©·å¤§
            q_values[~valid_mask] = -float('inf')
            
            # é€‰æ‹©æœ€é«˜Qå€¼çš„æœ‰æ•ˆåŠ¨ä½œ
            return q_values.argmax().item()

def visualize_board(board, rows=9, cols=9):
    """å¯è§†åŒ–æ‰«é›·æ£‹ç›˜"""
    symbols = {
        -2: "â– ",  # æœªæ‰“å¼€
        -1: "ğŸ’£", # åœ°é›·
        0: " ",   # ç©ºç™½
        1: "1ï¸âƒ£", 2: "2ï¸âƒ£", 3: "3ï¸âƒ£", 
        4: "4ï¸âƒ£", 5: "5ï¸âƒ£", 6: "6ï¸âƒ£", 
        7: "7ï¸âƒ£", 8: "8ï¸âƒ£"
    }
    
    print("+" + "---+" * cols)
    for i in range(rows):
        row_str = "|"
        for j in range(cols):
            cell = board[i * cols + j]
            row_str += f" {symbols[cell]} |"
        print(row_str)
        print("+" + "---+" * cols)

def play_game(agent, rows=9, cols=9):
    """ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ç©æ‰«é›·æ¸¸æˆ"""
    env = MineSweeperEnv(rows, cols, 10)
    state, _ = env.reset()
    done = False
    step = 0
    
    print("===== æ‰«é›·æ¸¸æˆå¼€å§‹! =====")
    
    while not done:
        step += 1
        action = agent.select_action(state)
        row, col = divmod(action, cols)
        
        print(f"\næ­¥éª¤ {step}: ç‚¹å‡»ä½ç½® ({row}, {col})")
        
        next_state, reward, done, _, _ = env.step(action)
        state = next_state
        
        # å¯è§†åŒ–å½“å‰æ£‹ç›˜
        visualize_board(state, rows, cols)
        
        # æ˜¾ç¤ºå¥–åŠ±
        if done:
            game_state = env.game.get_game_state()
            if game_state == 1:
                print("ğŸ‰ æ­å–œä½ èµ¢äº†! ğŸ‰")
            else:
                print("ğŸ’¥ è¸©åˆ°åœ°é›·äº†! æ¸¸æˆç»“æŸã€‚")
            print(f"æœ€ç»ˆå¥–åŠ±: {reward}")
        else:
            print(f"æœ¬æ¬¡å¥–åŠ±: {reward}")

def test_game(agent, rows=9, cols=9, mines=10):
    """
    æµ‹è¯•ä¸€å±€æ‰«é›·æ¸¸æˆï¼Œä½¿ç”¨è®­ç»ƒå¥½çš„DQNæ¨¡å‹è¿›è¡Œå†³ç­–å¹¶å¯è§†åŒ–æ£‹ç›˜çŠ¶æ€ã€‚
    """
    env = MineSweeperEnv(rows, cols, mines)
    state, _ = env.reset()
    done = False
    step = 0
    
    print("=" * 50)
    print(f"å¼€å§‹æ‰«é›·æ¸¸æˆ ({rows}x{cols}, {mines}ä¸ªåœ°é›·)")
    print("=" * 50)
    
    while not done:
        step += 1
        action = agent.select_action(state)
        row, col = divmod(action, cols)
        
        # æ£€æŸ¥åŠ¨ä½œæ˜¯å¦æœ‰æ•ˆ
        if state[action] != -2:
            print(f"âš ï¸ è­¦å‘Šï¼šAIé€‰æ‹©äº†æ— æ•ˆåŠ¨ä½œ ({row}, {col})ï¼è¯¥ä½ç½®å·²æ­å¼€")
        
        print(f"\næ­¥éª¤ {step}: ç‚¹å‡»ä½ç½® ({row}, {col})")
        
        next_state, reward, done, _, _ = env.step(action)
        state = next_state
        
        # å¯è§†åŒ–å½“å‰æ£‹ç›˜
        visualize_board(state, rows, cols)
        
        # æ˜¾ç¤ºå¥–åŠ±
        print(f"æœ¬æ¬¡å¥–åŠ±: {reward:.1f}")
        
        if done:
            game_state = env.game.get_game_state()
            if game_state == 1:
                print("\nğŸ‰ æ¸¸æˆèƒœåˆ©! æ­å–œAIè·èƒœ! ğŸ‰")
            else:
                print("\nğŸ’¥ æ¸¸æˆç»“æŸ! AIè¸©åˆ°åœ°é›·äº†! ğŸ’¥")
            
            # æ˜¾ç¤ºéšè—çš„åœ°é›·ä½ç½®
            hidden_board = env.game.get_hidden_board()
            flat_board = np.array(hidden_board).flatten()
            print("\nå®Œæ•´æ£‹ç›˜ï¼ˆæ˜¾ç¤ºåœ°é›·ä½ç½®ï¼‰:")
            visualize_board(flat_board, rows, cols)
            
            print(f"æœ€ç»ˆå¥–åŠ±: {reward:.1f}")
            print(f"æ€»æ­¥æ•°: {step}")
    
    return step, reward

if __name__ == "__main__":
    # è®¾ç½®æ£‹ç›˜å¤§å°
    rows, cols = 9, 9
    input_size = rows * cols
    output_size = rows * cols

    # åˆ›å»ºä»£ç†å¹¶åŠ è½½æ¨¡å‹
    agent = DQNAgent(input_size, output_size)
    base_dir = "./trainer/models/"
    model_name = "ddqn_model_MLP_step450000_20250530_183157.pth" # æ¯æ¬¡æµ‹è¯•æ—¶æ›¿æ¢ä¸ºæ¨¡å‹æ–‡ä»¶å
    path = f"{base_dir}{model_name}"
    agent.load_model(path)  # åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹

    # æµ‹è¯•æ¸¸æˆ
    test_game(agent, rows, cols, mines=10)

    # å¼€å§‹æ¸¸æˆ
    # play_game(agent, rows, cols)